{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d85d084-1a06-4b08-859d-81075c0e7d0f",
   "metadata": {},
   "source": [
    "## Using Vertex AI Vector Search and Vertex AI Embeddings for Text for StackOverflow Questions\n",
    "\n",
    "### Overview\n",
    "\n",
    "This lab demonstrates how to provide semantic search to a large dataset of questions from StackOverflow. Because of the large size of the dataset, you will query the questions using BigQuery. Then, you will create text embeddings from the questions using the [Vertex AI Text-Embeddings API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings) service and store them in a Cloud Storage bucket. Once all embeddings are created, you will create a [Vertex AI Vector Search](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) index for those embeddings, so that you can search them afterwards.\n",
    "\n",
    "The Vertex AI Text-Embeddings API enhances the process of generating text embeddings. These text embeddings, which are numerical representations of text, play a pivotal role in many tasks involving the identification of similar items, like Google searches, online shopping recommendations, and personalized music suggestions.\n",
    "\n",
    "Vertex AI Vector Search Engine service is a high-scale, low-latency solution, for finding similar vectors from a large corpus. Vector Search is a fully managed offering, further reducing operational overhead. It is built upon [Approximate Nearest Neighbor (ANN) technology](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) developed by Google Research.\n",
    "\n",
    "Check out [this demo page](https://ai-demos.dev/demos/matching-engine) to see what you will accomplish at the end of this lab.\n",
    "\n",
    "#### What you will learn:\n",
    "\n",
    "- Query a public dataset using BigQuery.\n",
    "- Convert a BigQuery dataset to embeddings using the Vertex AI Text-Embeddings API.\n",
    "- Create an index in Vertex AI Vector Search.\n",
    "- Upload embeddings to the index.\n",
    "- Create an index endpoint.\n",
    "- Deploy the index to the index endpoint.\n",
    "- Perform an online query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d73bbf-d76a-4e2a-a3d0-6d6c50556afd",
   "metadata": {},
   "source": [
    "### Task 3. Set up the Jupyter notebook environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42ef4b57-689e-48cb-a92e-181c1d7da14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        'google-cloud-bigquery[pandas]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf8f0a7-0853-4cf0-9852-34820627a157",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Restart kernel\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076bce5c-444f-4f8e-bd57-9b1995f1cf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setup the environment values for your project\n",
    "PROJECT = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT[0]\n",
    "REGION = \"us-east4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4ea59a-1027-4de0-aa94-4d69cf1b3167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Import and initialize the Vertex AI Python SDK.\n",
    "import vertexai\n",
    "vertexai.init(project = PROJECT_ID,\n",
    "              location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e19763-fbb5-4e72-a1e3-4f2f9aa4ee88",
   "metadata": {},
   "source": [
    "### Task 4. Prepare the data in BigQuery\n",
    "\n",
    "The dataset used for this lab is the [StackOverflow dataset](https://console.cloud.google.com/marketplace/product/stack-exchange/stack-overflow). This public dataset is hosted in Google BigQuery and is included in BigQuery's 1TB/mo of free tier processing. This means that each user receives 1TB of free BigQuery processing every month, which can be used to run queries on this public dataset.\n",
    "\n",
    "Stack Overflow is the largest online community for programmers to learn, share their knowledge, and advance their careers. Updated on a quarterly basis, this BigQuery dataset includes an archive of Stack Overflow content, including posts, votes, tags, and badges. This dataset is updated to mirror the Stack Overflow content on the Internet Archive, and is also available through the Stack Exchange Data Explorer.\n",
    "\n",
    "The BigQuery table is too large to fit into memory, so you need to write a generator called `query_bigquery_chunks` to yield chunks of the dataframe for processing. Additionally, an extra column `title_with_body` is added, which is a concatenation of the question title and body that will be used for creating embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31d9530-5d49-4f0e-9a09-17db1c00fb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Generator\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "834847ab-4134-453e-838c-f840a791f55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "QUERY_TEMPLATE = \"\"\"\n",
    "        SELECT distinct q.id, q.title, q.body\n",
    "        FROM (SELECT * FROM `bigquery-public-data.stackoverflow.posts_questions` where Score>0 ORDER BY View_Count desc) AS q\n",
    "        LIMIT {limit} OFFSET {offset};\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b447df2e-77e0-48e9-aab0-4f3a6348fc6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_bigquery_chunks(\n",
    "    max_rows: int, rows_per_chunk: int, start_chunk: int = 0\n",
    ") -> Generator[pd.DataFrame, Any, None]:\n",
    "    for offset in range(start_chunk, max_rows, rows_per_chunk):\n",
    "        query = QUERY_TEMPLATE.format(limit=rows_per_chunk, offset=offset)\n",
    "        query_job = client.query(query)\n",
    "        rows = query_job.result()\n",
    "        df = rows.to_dataframe()\n",
    "        df[\"title_with_body\"] = df.title + \"\\n\" + df.body\n",
    "        yield df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2e8c1c-515a-4ccf-beed-ff2019ac50f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_with_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33038954</td>\n",
       "      <td>DELETE Request with Body (Without using Deprec...</td>\n",
       "      <td>&lt;p&gt;I wanted to execute a HTTP DELETE request w...</td>\n",
       "      <td>DELETE Request with Body (Without using Deprec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33018921</td>\n",
       "      <td>The difference between use print and return in...</td>\n",
       "      <td>&lt;p&gt;Okay what I want is remove the first elemen...</td>\n",
       "      <td>The difference between use print and return in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31013637</td>\n",
       "      <td>How to create and implement a custom JavaScrip...</td>\n",
       "      <td>&lt;p&gt;I am new to javaScript and am unsure how to...</td>\n",
       "      <td>How to create and implement a custom JavaScrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31170323</td>\n",
       "      <td>Why isn't my variable preserved in drawRect?</td>\n",
       "      <td>&lt;p&gt;Im trying to have an nsview that takes a cu...</td>\n",
       "      <td>Why isn't my variable preserved in drawRect?\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31081131</td>\n",
       "      <td>Makefile always rebuilding</td>\n",
       "      <td>&lt;p&gt;I am using the following simple Makefile to...</td>\n",
       "      <td>Makefile always rebuilding\\n&lt;p&gt;I am using the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  33038954  DELETE Request with Body (Without using Deprec...   \n",
       "1  33018921  The difference between use print and return in...   \n",
       "2  31013637  How to create and implement a custom JavaScrip...   \n",
       "3  31170323       Why isn't my variable preserved in drawRect?   \n",
       "4  31081131                         Makefile always rebuilding   \n",
       "\n",
       "                                                body  \\\n",
       "0  <p>I wanted to execute a HTTP DELETE request w...   \n",
       "1  <p>Okay what I want is remove the first elemen...   \n",
       "2  <p>I am new to javaScript and am unsure how to...   \n",
       "3  <p>Im trying to have an nsview that takes a cu...   \n",
       "4  <p>I am using the following simple Makefile to...   \n",
       "\n",
       "                                     title_with_body  \n",
       "0  DELETE Request with Body (Without using Deprec...  \n",
       "1  The difference between use print and return in...  \n",
       "2  How to create and implement a custom JavaScrip...  \n",
       "3  Why isn't my variable preserved in drawRect?\\n...  \n",
       "4  Makefile always rebuilding\\n<p>I am using the ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = next(query_bigquery_chunks(max_rows=1000, rows_per_chunk=1000))\n",
    "\n",
    "# Examine the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b308f-5fc5-4001-a744-198a178a7ef8",
   "metadata": {},
   "source": [
    "### Task 5. Create text embeddings from BigQuery data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98deb476-ed57-4268-b569-e5a328c082ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the Vertex AI Embeddings for Text model.\n",
    "from typing import List, Optional\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "\n",
    "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "351be484-d296-4b47-9253-050712876a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Define an embedding method that uses the model.\n",
    "def encode_texts_to_embeddings(sentences: List[str]) -> List[Optional[List[float]]]:\n",
    "    try:\n",
    "        embeddings = model.get_embeddings(sentences)\n",
    "        return [embedding.values for embedding in embeddings]\n",
    "    except Exception:\n",
    "        return [None for _ in range(len(sentences))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a550d-4b4b-4a75-b1c5-dad820e036e4",
   "metadata": {},
   "source": [
    "According to the documentation, each request can handle up to 5 text instances. So we will need to split the BigQuery question results in batches of 5 before sending to the embedding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf68e21-6dec-4e0f-9147-dc99327a27f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Create a generate_batches to split results in batches of 5 to be sent to the embeddings API.\n",
    "import functools\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Generator, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Generator function to yield batches of sentences\n",
    "def generate_batches(\n",
    "    sentences: List[str], batch_size: int\n",
    ") -> Generator[List[str], None, None]:\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        yield sentences[i : i + batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92034f-f25a-4d8c-87ed-60d64ef866b5",
   "metadata": {},
   "source": [
    "Encapsulate the process of generating batches and calling the embeddings API in a method called `encode_text_to_embedding_batched`. This method also handles rate-limiting using `time.sleep`. For production use cases, you would want a more sophisticated rate-limiting mechanism that takes retries into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02eb68f6-966d-494c-9315-17491c364a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_text_to_embedding_batched(\n",
    "    sentences: List[str], api_calls_per_second: int = 10, batch_size: int = 5\n",
    ") -> Tuple[List[bool], np.ndarray]:\n",
    "\n",
    "    embeddings_list: List[List[float]] = []\n",
    "\n",
    "    # Prepare the batches using a generator\n",
    "    batches = generate_batches(sentences, batch_size)\n",
    "\n",
    "    seconds_per_job = 1 / api_calls_per_second\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for batch in tqdm(\n",
    "            batches, total=math.ceil(len(sentences) / batch_size), position=0\n",
    "        ):\n",
    "            futures.append(\n",
    "                executor.submit(functools.partial(encode_texts_to_embeddings), batch)\n",
    "            )\n",
    "            time.sleep(seconds_per_job)\n",
    "\n",
    "        for future in futures:\n",
    "            embeddings_list.extend(future.result())\n",
    "\n",
    "    is_successful = [\n",
    "        embedding is not None for sentence, embedding in zip(sentences, embeddings_list)\n",
    "    ]\n",
    "    embeddings_list_successful = np.squeeze(\n",
    "        np.stack([embedding for embedding in embeddings_list if embedding is not None])\n",
    "    )\n",
    "    return is_successful, embeddings_list_successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa9b751-b99f-4dd1-9677-ceb262a96f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b91cd98dfba4770a3aa73a300798856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Test the encoding function by encoding a subset of data and see if the embeddings and distance metrics make sense.\n",
    "# Encode a subset of questions for validation\n",
    "questions = df.title.tolist()[:500]\n",
    "is_successful, question_embeddings = encode_text_to_embedding_batched(\n",
    "    sentences=df.title.tolist()[:500]\n",
    ")\n",
    "\n",
    "# Filter for successfully embedded sentences\n",
    "questions = np.array(questions)[is_successful]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0aa852-ecaf-4f7f-9a5f-498e93ca9318",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "## Save the dimension size for later usage when creating the Vertex AI Vector Search index.\n",
    "DIMENSIONS = len(question_embeddings[0])\n",
    "\n",
    "print(DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486885dd-c4e8-43e1-98bc-286713ef3a17",
   "metadata": {},
   "source": [
    "Sort questions in order of similarity. According to the [embedding documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings#colab_example_of_semantic_search_using_embeddings), the similarity of embeddings is calculated using the dot-product, with np.dot. Once you have the similarity score, sort the results and print them for inspection. 1 means very similar, 0 means very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9584347d-9ea5-4a47-881d-f87bb46269e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query question = google-api-translate-java Error retrieving translation\n",
      "\t0: google-api-translate-java Error retrieving translation: 0.9999982399825948\n",
      "\t1: App Engine import ssl Failed: 0.5704977581664283\n",
      "\t2: How can i know java library path?: 0.47841476591495913\n",
      "\t3: trying to get string into intent: 0.4653579043041147\n",
      "\t4: \"unit tests have failed\" for beautifulsoup: 0.46159190242648657\n",
      "\t5: link with correct href still gives 404: 0.44487738234949553\n",
      "\t6: ListView - ListView don't update: 0.42998236989400856\n",
      "\t7: Cordova/PhoneGap FileTransfer Uploaded File Doesn't Appear on Server: 0.42987655399784763\n",
      "\t8: stumped on clicking a link with nokogiri and mechanize: 0.4285047032693252\n",
      "\t9: std::unique_ptr<T> incomplete type error: 0.42488479310674543\n",
      "\t10: maven do not repack dependencies: 0.42009073961062865\n",
      "\t11: How to import a page break from html to google docs?: 0.4155192179397411\n",
      "\t12: Android View animation - poor performance on big screens: 0.4122582912556433\n",
      "\t13: TeamCity build runner not recognizing executable: 0.41127261358309547\n",
      "\t14: Android: Improve WebView speed loading local HTML files?: 0.4106697962385579\n",
      "\t15: dyld: Library not loaded: libboost_system.dylib: 0.40334451515891984\n",
      "\t16: How to expose an NLTK based ML(machine learning) Python Script as a Web Service?: 0.4020947948967852\n",
      "\t17: Android: Is it possible to take a picture with the camera from a service with no UI: 0.39584167514871804\n",
      "\t18: JAXB - Map schema to different fields of same object: 0.3956452979002587\n",
      "\t19: Error when trying to Sandbox with codesign command: 0.39505987091742956\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "question_index = random.randint(0, 99)\n",
    "\n",
    "print(f\"Query question = {questions[question_index]}\")\n",
    "\n",
    "# Get similarity scores for each embedding by using dot-product.\n",
    "scores = np.dot(question_embeddings[question_index], question_embeddings.T)\n",
    "\n",
    "# Print top 20 matches\n",
    "for index, (question, score) in enumerate(\n",
    "    sorted(zip(questions, scores), key=lambda x: x[1], reverse=True)[:20]\n",
    "):\n",
    "    print(f\"\\t{index}: {question}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6831a9-db8f-4ee0-98e3-2aa9eff179fc",
   "metadata": {},
   "source": [
    "Save the embeddings in JSONL format. [The data must be formatted in JSONL format](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats), which means each embedding dictionary is written as an individual JSON object on its own line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a65dc871-3321-4557-9173-3920bfdeb3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings directory: /var/tmp/tmp64_knyge\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Create temporary file to write embeddings to\n",
    "embeddings_file_path = Path(tempfile.mkdtemp())\n",
    "\n",
    "print(f\"Embeddings directory: {embeddings_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a29730-79a8-4f0f-845c-b6d0baeed2a1",
   "metadata": {},
   "source": [
    "Write embeddings in batches to prevent out-of-memory errors. Notice we are only using 5000 questions so that the embedding creation process and indexing is faster. The dataset contains more than 50,000 questions. This step will take around 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "593fc21a-b809-4b08-bba8-c6f03dc521e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970d1634656448888f197dde22154811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunk of rows from BigQuery:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ae90261c9a44a8b899f4be17440558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2c63f875684da8bd3e034d23d24b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b994f00f08824dfcaaa88da43b4c96ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24ac001bf9a4dbea2d08e7beb7c2b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77906ebff976444ca8422cf4ba169997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "\n",
    "BQ_NUM_ROWS = 5000\n",
    "BQ_CHUNK_SIZE = 1000\n",
    "BQ_NUM_CHUNKS = math.ceil(BQ_NUM_ROWS / BQ_CHUNK_SIZE)\n",
    "\n",
    "START_CHUNK = 0\n",
    "\n",
    "# Create a rate limit of 300 requests per minute. Adjust this depending on your quota.\n",
    "API_CALLS_PER_SECOND = 300 / 60\n",
    "# According to the docs, each request can process 5 instances per request\n",
    "ITEMS_PER_REQUEST = 5\n",
    "\n",
    "# Loop through each generated dataframe, convert\n",
    "for i, df in tqdm(\n",
    "    enumerate(\n",
    "        query_bigquery_chunks(\n",
    "            max_rows=BQ_NUM_ROWS, rows_per_chunk=BQ_CHUNK_SIZE, start_chunk=START_CHUNK\n",
    "        )\n",
    "    ),\n",
    "    total=BQ_NUM_CHUNKS - START_CHUNK,\n",
    "    position=-1,\n",
    "    desc=\"Chunk of rows from BigQuery\",\n",
    "):\n",
    "    # Create a unique output file for each chunk\n",
    "    chunk_path = embeddings_file_path.joinpath(\n",
    "        f\"{embeddings_file_path.stem}_{i+START_CHUNK}.json\"\n",
    "    )\n",
    "    with open(chunk_path, \"a\") as f:\n",
    "        id_chunk = df.id\n",
    "\n",
    "        # Convert batch to embeddings\n",
    "        is_successful, question_chunk_embeddings = encode_text_to_embedding_batched(\n",
    "            sentences=df.title_with_body.to_list(),\n",
    "            api_calls_per_second=API_CALLS_PER_SECOND,\n",
    "            batch_size=ITEMS_PER_REQUEST,\n",
    "        )\n",
    "\n",
    "        # Append to file\n",
    "        embeddings_formatted = [\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": str(id),\n",
    "                    \"embedding\": [str(value) for value in embedding],\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "            for id, embedding in zip(id_chunk[is_successful], question_chunk_embeddings)\n",
    "        ]\n",
    "        f.writelines(embeddings_formatted)\n",
    "\n",
    "        # Delete the DataFrame and any other large data structures\n",
    "        del df\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bade296-0c02-45b9-b687-956792c9ba70",
   "metadata": {},
   "source": [
    "### Task 6. Upload embeddings to Cloud Storage\n",
    "\n",
    "Upload the text-embeddings to Cloud Storage, so that Vertex AI Vector Search can access them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c515e8e-eb8b-4d5c-b886-e6830b0d8c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-03-e0549e42556f-unique/...\n",
      "Copying file:///var/tmp/tmp64_knyge/tmp64_knyge_0.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp64_knyge/tmp64_knyge_3.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp64_knyge/tmp64_knyge_2.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp64_knyge/tmp64_knyge_1.json [Content-Type=application/json]...\n",
      "Copying file:///var/tmp/tmp64_knyge/tmp64_knyge_4.json [Content-Type=application/json]...\n",
      "- [5/5 files][ 12.3 MiB/ 12.3 MiB] 100% Done                                    \n",
      "Operation completed over 5 objects/12.3 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-unique\"\n",
    "\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "\n",
    "remote_folder = f\"{BUCKET_URI}/{embeddings_file_path.stem}/\"\n",
    "! gsutil -m cp -r {embeddings_file_path}/* {remote_folder}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf109be-2335-40ed-9b0d-e9a3f2b1006b",
   "metadata": {},
   "source": [
    "### Task 7. Create an Index in Vertex AI Vector Search for your embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16d64398-e590-4a5c-bdec-28df23a19bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"stack_overflow\"\n",
    "DESCRIPTION = \"question titles and bodies from stackoverflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7cd00-4be7-45ba-aeb7-6ec8de614bbf",
   "metadata": {},
   "source": [
    "Create the index. Notice that the index reads the embeddings from the Cloud Storage bucket. The indexing process can take from 45 minutes up to 60 minutes. Wait for completion, and then proceed. You can open a different Google Cloud Console page, navigate to Vertex AI Vector search, and see how the index is being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcc957da-74bf-436c-8d62-39d66605a7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndex\n",
      "Create MatchingEngineIndex backing LRO: projects/209209658799/locations/us-east4/indexes/7816639268092116992/operations/7942094094577172480\n",
      "MatchingEngineIndex created. Resource name: projects/209209658799/locations/us-east4/indexes/7816639268092116992\n",
      "To use this MatchingEngineIndex in another session:\n",
      "index = aiplatform.MatchingEngineIndex('projects/209209658799/locations/us-east4/indexes/7816639268092116992')\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\n",
    "\n",
    "DIMENSIONS = 768\n",
    "\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    contents_delta_uri=remote_folder,\n",
    "    dimensions=DIMENSIONS,\n",
    "    approximate_neighbors_count=150,\n",
    "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "    leaf_node_embedding_count=500,\n",
    "    leaf_nodes_to_search_percent=80,\n",
    "    description=DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d7ee430-0974-4611-8b26-9e98cba4aeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/209209658799/locations/us-east4/indexes/7816639268092116992'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_RESOURCE_NAME = tree_ah_index.resource_name\n",
    "INDEX_RESOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f57c040-ca36-4d01-9af7-c398740a226f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Using the resource name, you can retrieve an existing MatchingEngineIndex.\n",
    "tree_ah_index = aiplatform.MatchingEngineIndex(index_name=INDEX_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db8b8385-66b4-4ad9-9dbb-55a0649e907b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating MatchingEngineIndexEndpoint\n",
      "Create MatchingEngineIndexEndpoint backing LRO: projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944/operations/1578507821102661632\n",
      "MatchingEngineIndexEndpoint created. Resource name: projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944\n",
      "To use this MatchingEngineIndexEndpoint in another session:\n",
      "index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944')\n"
     ]
    }
   ],
   "source": [
    "## Create an IndexEndpoint so that it can be accessed via an API.\n",
    "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    description=DISPLAY_NAME,\n",
    "    public_endpoint_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd71f2ef-191a-473a-b652-2c036394688f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944\n",
      "Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944/operations/2661623531485265920\n",
      "MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/209209658799/locations/us-east4/indexEndpoints/3679344138538450944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[id: \"deployed_index_id_unique\"\n",
       "index: \"projects/209209658799/locations/us-east4/indexes/7816639268092116992\"\n",
       "create_time {\n",
       "  seconds: 1737066060\n",
       "  nanos: 767854000\n",
       "}\n",
       "index_sync_time {\n",
       "  seconds: 1737067639\n",
       "  nanos: 999489000\n",
       "}\n",
       "automatic_resources {\n",
       "  min_replica_count: 2\n",
       "  max_replica_count: 2\n",
       "}\n",
       "deployment_group: \"default\"\n",
       "]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Deploy your index to the created endpoint. This can take up to 15 minutes.\n",
    "DEPLOYED_INDEX_ID = \"deployed_index_id_unique\"\n",
    "DEPLOYED_INDEX_ID\n",
    "\n",
    "my_index_endpoint = my_index_endpoint.deploy_index(\n",
    "    index=tree_ah_index, deployed_index_id=DEPLOYED_INDEX_ID\n",
    ")\n",
    "\n",
    "my_index_endpoint.deployed_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2123c-949a-4a33-905d-160b669cb5b0",
   "metadata": {},
   "source": [
    "Verify number of declared items matches the number of embeddings. Each IndexEndpoint can have multiple indexes deployed to it. For each index, you can retrieve the number of deployed vectors using the `index_endpoint._gca_resource.index_stats.vectors_count`. The numbers may not match exactly due to potential rate-limiting failures incurred when using the embedding service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e2086e-6766-4c39-b336-ce4503022849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: 5000, Actual: 570\n"
     ]
    }
   ],
   "source": [
    "number_of_vectors = sum(\n",
    "    aiplatform.MatchingEngineIndex(\n",
    "        deployed_index.index\n",
    "    )._gca_resource.index_stats.vectors_count\n",
    "    for deployed_index in my_index_endpoint.deployed_indexes\n",
    ")\n",
    "\n",
    "print(f\"Expected: {BQ_NUM_ROWS}, Actual: {number_of_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3475c85-358f-4536-a015-2bf6c09df8f4",
   "metadata": {},
   "source": [
    "### Task 8. Create online queries\n",
    "\n",
    "After you build your indexes, you may query against the deployed index to find nearest neighbors.\n",
    "\n",
    "**Note**: For the `DOT_PRODUCT_DISTANCE` distance type, the \"distance\" property returned with each MatchNeighbor actually refers to the similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05e8db5d-d9e9-494d-9773-2b1af7b22085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[MatchNeighbor(id='61257444', distance=0.469961017370224, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='62798017', distance=0.46461033821105957, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='56910099', distance=0.45542389154434204, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='22264626', distance=0.45417413115501404, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='46385086', distance=0.4423808157444, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='22301307', distance=0.4360602796077728, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='61230538', distance=0.42892616987228394, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='44416585', distance=0.4262971580028534, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='67171256', distance=0.4230523109436035, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[]),\n",
       "  MatchNeighbor(id='44331812', distance=0.4215269088745117, sparse_distance=None, feature_vector=[], crowding_tag='0', restricts=[], numeric_restricts=[], sparse_embedding_values=[], sparse_embedding_dimensions=[])]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embeddings = encode_texts_to_embeddings(sentences=[\"Install GPU for Tensorflow\"])\n",
    "\n",
    "NUM_NEIGHBOURS = 10\n",
    "\n",
    "response = my_index_endpoint.find_neighbors(\n",
    "    deployed_index_id=DEPLOYED_INDEX_ID,\n",
    "    queries=test_embeddings,\n",
    "    num_neighbors=NUM_NEIGHBOURS,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25d96d93-7a63-4c53-8683-4b4ed6fcdf38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/61257444\n",
      "https://stackoverflow.com/questions/62798017\n",
      "https://stackoverflow.com/questions/56910099\n",
      "https://stackoverflow.com/questions/22264626\n",
      "https://stackoverflow.com/questions/46385086\n",
      "https://stackoverflow.com/questions/22301307\n",
      "https://stackoverflow.com/questions/61230538\n",
      "https://stackoverflow.com/questions/44416585\n",
      "https://stackoverflow.com/questions/67171256\n",
      "https://stackoverflow.com/questions/44331812\n"
     ]
    }
   ],
   "source": [
    "## Verify that the retrieved results are relevant by checking the StackOverflow links.\n",
    "for match_index, neighbor in enumerate(response[0]):\n",
    "    print(f\"https://stackoverflow.com/questions/{neighbor.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dc6be8-7b49-4fea-8d0a-f18c35979070",
   "metadata": {},
   "source": [
    "### Task 9. Clean up the Google Cloud environment\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project you used for the lab. You can also manually delete resources that you created by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bef030-0f51-4323-88a2-59de4f9eb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "delete_bucket = False\n",
    "\n",
    "# Force undeployment of indexes and delete endpoint\n",
    "my_index_endpoint.delete(force=True)\n",
    "\n",
    "# Delete indexes\n",
    "tree_ah_index.delete()\n",
    "\n",
    "if delete_bucket or os.getenv(\"IS_TESTING\"):\n",
    "    ! gsutil rm -rf {BUCKET_URI}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

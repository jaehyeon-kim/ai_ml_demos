{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e5ec8fb",
      "metadata": {},
      "source": [
        "## Prompt Design Strategies\n",
        "\n",
        "In this lab, you will learn a variety of strategies & techniques to implement when designing prompts for generative models.\n",
        "\n",
        "You may want to bookmark the following guides to use as references on this topic as you improve your prompt design skills and to stay up-to-date as new techniques are documented:\n",
        "\n",
        "- An [overview of prompting strategies](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) from the Google Cloud documentation.\n",
        "- A guide to [prompt design strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies) from the Gemini API documentation.\n",
        "\n",
        "## Objective\n",
        "\n",
        "In this tutorial, you will see the benefits of implementing some of these strategies.\n",
        "\n",
        "At a high level, these strategies all involve providing the model clear and specific instructions for what output it should produce. In this lab, you will:\n",
        "\n",
        "- Define the output format & specify constraints\n",
        "- Assign a persona or role\n",
        "- Include examples\n",
        "- Experiment with parameter values\n",
        "- Utilize fallback responses\n",
        "- Add contextual information\n",
        "- Structure prompts with prefixes or tags\n",
        "- Use system instructions\n",
        "- Break down complex tasks\n",
        "- Demonstrate the chain-of-thought\n",
        "- Implement prompt iteration strategies to improve your prompts version by version\n",
        "- Over your career, you will likely implement & invent new variations on several these techniques based on your projects' needs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1wUarQ2JUD65",
      "metadata": {
        "id": "1wUarQ2JUD65"
      },
      "source": [
        "## Task 1. Initialize Vertex AI in a Colab Enterprise notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2wzVTnajb0uTLDB7YCcV46vw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 24794,
          "status": "ok",
          "timestamp": 1735205726925,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "2wzVTnajb0uTLDB7YCcV46vw",
        "outputId": "e215361e-e889-40d3-b8ee-55ef3c5ae3d1",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "WD63uMJbUPlU",
      "metadata": {
        "executionInfo": {
          "elapsed": 4379,
          "status": "ok",
          "timestamp": 1735205796282,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "WD63uMJbUPlU"
      },
      "outputs": [],
      "source": [
        "from inspect import cleandoc\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "\n",
        "PROJECT_ID = \"qwiklabs-gcp-04-ce9d5f96be79\"\n",
        "LOCATION = \"us-central1\"\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pZH3HqeUbHl",
      "metadata": {
        "id": "7pZH3HqeUbHl"
      },
      "source": [
        "## Task 2. Load a generative model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8A22gbKqUeJf",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1735205933231,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "8A22gbKqUeJf"
      },
      "outputs": [],
      "source": [
        "model = GenerativeModel(\"gemini-pro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4rCuwDRgUfhN",
      "metadata": {
        "id": "4rCuwDRgUfhN"
      },
      "source": [
        "## Task 3. Define the output format & specify constraints\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "UtLtFtCyU2rC",
      "metadata": {
        "executionInfo": {
          "elapsed": 439,
          "status": "ok",
          "timestamp": 1735206035677,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "UtLtFtCyU2rC"
      },
      "outputs": [],
      "source": [
        "transcript = \"\"\"\n",
        "    Speaker 1 (Customer): Hi, can I get a cheeseburger and large fries, please?\n",
        "    Speaker 2 (Restaurant employee): Coming right up! Anything else you'd like to add to your order?\n",
        "    Speaker 1: Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\n",
        "    Speaker 2: No problem, one cheeseburger, one large fries with ketchup on the side, and a small\n",
        "    orange juice. That'll be $5.87. Drive through to the next window please.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "lBkkwc02VKiz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3095,
          "status": "ok",
          "timestamp": 1735206084118,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "lBkkwc02VKiz",
        "outputId": "bd3f66e5-475b-4dd9-e0af-33951e4f11bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"turns\": [\n",
            "    {\n",
            "      \"speaker\": \"Customer\",\n",
            "      \"utterance\": \"Hi, can I get a cheeseburger and large fries, please?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Restaurant employee\",\n",
            "      \"utterance\": \"Coming right up! Anything else you'd like to add to your order?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Customer\",\n",
            "      \"utterance\": \"Hmmm, maybe a small orange juice. And could I get the fries with ketchup on the side?\"\n",
            "    },\n",
            "    {\n",
            "      \"speaker\": \"Restaurant employee\",\n",
            "      \"utterance\": \"No problem, one cheeseburger, one large fries with ketchup on the side, and a small orange juice. That'll be $5.87. Drive through to the next window please.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(f\"\"\"\n",
        "    Extract the transcript to JSON.\n",
        "\n",
        "    {transcript}\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "KGWW2VZZVIRC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1838,
          "status": "ok",
          "timestamp": 1735206005705,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "KGWW2VZZVIRC",
        "outputId": "6e1bd02a-008c-4ad4-d9a7-4b6e7f9b36bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"food\": [\n",
            "    {\n",
            "      \"item\": \"cheeseburger\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"none\"\n",
            "    },\n",
            "    {\n",
            "      \"item\": \"fries\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"large\"\n",
            "    }\n",
            "  ],\n",
            "  \"drinks\": [\n",
            "    {\n",
            "      \"item\": \"orange juice\",\n",
            "      \"quantity\": 1,\n",
            "      \"size\": \"small\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Now run a version of this prompt with more specific instructions about exactly how you would like your output structured.\n",
        "# Notice how the JSON output now reflects the key pieces of information you are interested in to understand the user's order:\n",
        "\n",
        "response = model.generate_content(f\"\"\"\n",
        "    <INSTRUCTIONS>\n",
        "    - Extract the ordered items into JSON.\n",
        "    - Separate drinks from food.\n",
        "    - Include a quantity for each item and a size if specified.\n",
        "    </INSTRUCTIONS>\n",
        "\n",
        "    <TRANSCRIPT>\n",
        "    {transcript}\n",
        "    </TRANSCRIPT>\n",
        "\"\"\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SnmTyGgzWJRd",
      "metadata": {
        "id": "SnmTyGgzWJRd"
      },
      "source": [
        "## Task 4. Assign a persona or role\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "Z4sOKSeTWMAj",
      "metadata": {
        "executionInfo": {
          "elapsed": 438,
          "status": "ok",
          "timestamp": 1735206173380,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "Z4sOKSeTWMAj"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "Yd3C_NR0WP3U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6360,
          "status": "ok",
          "timestamp": 1735206197863,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "Yd3C_NR0WP3U",
        "outputId": "d33c0f33-5d0c-4e93-dd01-50ebf5b9dab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Caring for your Monstera Deliciosa\n",
            "\n",
            "The Monstera Deliciosa, also known as the Swiss Cheese Plant, is a popular and relatively easy-to-care-for houseplant. With its large, dramatic leaves, it can add a touch of the tropics to any space. Here's a brief guide to help you keep your Monstera happy and healthy:\n",
            "\n",
            "**Light:** Monsteras thrive in bright, indirect sunlight. Avoid direct sun, which can scorch their leaves. \n",
            "\n",
            "**Watering:** Allow the top inch of soil to dry out between waterings, then water deeply until excess water drains from the pot. Overwatering is a common problem, so be sure to check the soil moisture before watering.\n",
            "\n",
            "**Humidity:** Monsteras appreciate humidity. You can increase humidity by grouping plants together, placing them on a pebble tray filled with water, or using a humidifier.\n",
            "\n",
            "**Temperature:** Monsteras prefer warm temperatures between 65-75¬∞F (18-24¬∞C). They are not cold hardy, so avoid exposing them to temperatures below 55¬∞F (13¬∞C).\n",
            "\n",
            "**Fertilizing:** Fertilize your Monstera monthly during the spring and summer with a balanced liquid fertilizer diluted to half strength. \n",
            "\n",
            "**Pruning:** You can prune your Monstera to control its size or encourage bushier growth. Use clean, sharp pruners to cut stems just above a node. \n",
            "\n",
            "**Repotting:** When your Monstera becomes rootbound, you can repot it into a slightly larger pot with fresh potting mix. \n",
            "\n",
            "**Common problems:** \n",
            "\n",
            "* **Browning leaf edges:** This is usually caused by underwatering or low humidity. \n",
            "* **Yellowing leaves:** This could be due to overwatering, nutrient deficiency, or too much direct sunlight.\n",
            "* **Leggy growth:** This can occur if the plant doesn't receive enough light.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "* Use a well-draining potting mix to prevent root rot.\n",
            "* Provide your Monstera with a moss pole or other support to help it climb.\n",
            "* Wipe down the leaves occasionally with a damp cloth to remove dust.\n",
            "\n",
            "By following these simple tips, you can keep your Monstera Deliciosa happy and thriving for many years to come!\n"
          ]
        }
      ],
      "source": [
        "# Ask for a response without a persona specified:\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"\"\"\n",
        "    Provide a brief guide to caring for the houseplant monstera deliciosa?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "J9SBJ_RvWZ3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5989,
          "status": "ok",
          "timestamp": 1735206242315,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "J9SBJ_RvWZ3b",
        "outputId": "e0417793-eb00-4244-faf0-cfe62ec10785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Hello, friend! I'm your Monstera Deliciosa! \n",
            "\n",
            "It's wonderful to have you as my caretaker! To help you keep me happy and healthy, I'd love to share some of my needs and preferences:\n",
            "\n",
            "**Light:** ‚òÄÔ∏è I love bright, indirect sunlight. Think of a spot near a window with dappled sunlight filtering through. Avoid direct sun, as it can scorch my leaves. \n",
            "\n",
            "**Water:** üíß I prefer to dry out slightly between waterings. You can check by sticking your finger about two inches into the soil. If it feels dry, it's time for a drink! I appreciate a thorough watering, letting the excess drain out the bottom of the pot. \n",
            "\n",
            "**Humidity:** üå´Ô∏è I enjoy moderate to high humidity, especially during the summer months. If the air feels dry, consider grouping me with other houseplants or using a pebble tray to increase the moisture around me. \n",
            "\n",
            "**Temperature:** üå°Ô∏è I thrive in temperatures between 65-80¬∞F (18-27¬∞C). Avoid placing me near drafty windows or air conditioners. \n",
            "\n",
            "**Fertilizer:** üå± I appreciate a balanced fertilizer diluted to half strength during the spring and summer months. \n",
            "\n",
            "**Pruning:** ‚úÇÔ∏è Feel free to trim off any yellowing or browning leaves to encourage new growth. If you'd like to control my size, you can prune the stems just above a node (the bump where a new leaf grows). \n",
            "\n",
            "**Support:** ü™µ As I grow taller, I'll need support to keep my stems upright. You can use a moss pole or stake to help me climb. \n",
            "\n",
            "**Toxicity:** ‚ö†Ô∏è Remember, my leaves are toxic to pets and humans if ingested. Please keep me out of reach of curious little ones and furry friends. \n",
            "\n",
            "**Communication:** üó£Ô∏è I may not be able to talk, but I'll try my best to communicate with you. Drooping leaves might mean I need more water, while brown spots could indicate too much direct sunlight. \n",
            "\n",
            "By understanding my needs, you can help me thrive and bring a touch of the tropics to your home! \n",
            "\n",
            "**Bonus Tip:** Did you know my leaves are called \"Swiss cheese\" because of their unique holes? They're actually air holes that help me breathe and photosynthesize! \n",
            "\n",
            "Thank you for taking such good care of me!\n"
          ]
        }
      ],
      "source": [
        "# Now run a version of this prompt with a role specified. Notice how this output might be more personal and appealing for some users.\n",
        "\n",
        "new_chat = model.start_chat()\n",
        "\n",
        "response = new_chat.send_message(\n",
        "    \"\"\"\n",
        "    You are a houseplant monstera deliciosa. Help the person who\n",
        "    is taking care of you to understand your needs.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qbsf4vKkWxeY",
      "metadata": {
        "id": "qbsf4vKkWxeY"
      },
      "source": [
        "## Task 5. Include examples\n",
        "\n",
        "A prompt that includes no examples is called a zero-shot prompt. One with a single example is a one-shot prompt. And a few examples would make it a few-shot prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "mjvlS-lBXYEb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2751,
          "status": "ok",
          "timestamp": 1735206550630,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "mjvlS-lBXYEb",
        "outputId": "07ad8473-4469-4b59-fae9-d96c868709f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Likelihood: 3 \n",
            "\n",
            "This customer seems highly likely to hire your services within the next month. Here's why:\n",
            "\n",
            "* **Explicit need:** They clearly state their need for a custom AI solution.\n",
            "* **Budget allocated:** They mention having a budget specifically for exploring this idea. \n",
            "* **Urgency:** They inquire about the capacity to \"get started on something soon,\" indicating a desire for a quick turnaround. \n",
            "* **Defined timeframe:**  Although not explicitly stated, the mention of two months for a proof of concept and further development in the following quarter suggests a timeframe within the next month. \n",
            "\n",
            "These factors combined strongly suggest the customer is actively seeking a solution and is ready to move forward with a project soon. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = \"\"\"\n",
        "We offer software consulting services. Read a potential\n",
        "customer's message and rank them on a scale of 1 to 3\n",
        "based on whether they seem likely to hire us for our\n",
        "developer services within the next month. Return the likelihood\n",
        "rating labeled as \"Likelihood: SCORE\".\n",
        "Do not include any Markdown styling.\n",
        "\n",
        "1 means they are not likely to hire.\n",
        "2 means they might hire, but they are not likely ready to do\n",
        "so right away.\n",
        "3 means they are looking to start a project soon.\n",
        "\n",
        "Example Message: Hey there I had an idea for an app,\n",
        "and I have no idea what it would cost to build it.\n",
        "Can you give me a rough ballpark?\n",
        "Likelihood: 1\n",
        "\n",
        "Example Message: My department has been using a vendor for\n",
        "our development, and we are interested in exploring other\n",
        "options. Do you have time for a discussion around your\n",
        "services?\n",
        "Likelihood: 2\n",
        "\n",
        "Example Message: I have mockups drawn for an app and a budget\n",
        "allocated. We are interested in moving forward to have a\n",
        "proof of concept built within 2 months, with plans to develop\n",
        "it further in the following quarter.\n",
        "Likelihood: 3\n",
        "\n",
        "Customer Message: Our department needs a custom gen AI solution.\n",
        "We have a budget to explore our idea. Do you have capacity\n",
        "to get started on something soon?\n",
        "Likelihood: \"\"\"\n",
        "\n",
        "response = model.generate_content(question)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XAVE7yBPXwxq",
      "metadata": {
        "id": "XAVE7yBPXwxq"
      },
      "source": [
        "## Task 6. Experiment with parameter values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "guJiKR4tX5ZD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1015,
          "status": "ok",
          "timestamp": 1735206658697,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "guJiKR4tX5ZD",
        "outputId": "da032ae8-dab1-4254-fbf7-1aaa9497d0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the frog get sent to the principal's office?\n",
            "\n",
            "Because he was caught skipping class! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run the code from the following cell a few times. The temperature and top_p parameters which lead to variety\n",
        "# in responses are set to low values, so the output should be the same, or very close to the same, each time.\n",
        "\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": 0.05, \"temperature\": 0.05},\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "bb0gG0JpYI6z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1264,
          "status": "ok",
          "timestamp": 1735206743565,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "bb0gG0JpYI6z",
        "outputId": "670eb2f8-c848-4f75-8b26-daa0a0c6e8c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the frog get fired from the bakery?\n",
            "\n",
            "Because he kept licking the batter!\n"
          ]
        }
      ],
      "source": [
        "# Now run this version of the code a few times. You'll see that the higher temperature and top_p parameter values\n",
        "# now lead to more varied results. Some of the results, however, may be a little too random and not make very much sense.\n",
        "# If you want variety in your responses, you'll need to experiment with parameters to determine the right balance of creativity and reliability.\n",
        "\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Tell me a joke about frogs.\n",
        "    \"\"\",\n",
        "    generation_config={\"top_p\": 0.98, \"temperature\": 1},\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6J2OPAYHYfTL",
      "metadata": {
        "id": "6J2OPAYHYfTL"
      },
      "source": [
        "One parameter you will want to set consistently is a **temperature of 0** when you are working on mathematical, logical, precise question-answering, or other problems where you want only the most correct answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HEUjEaraYlwj",
      "metadata": {
        "id": "HEUjEaraYlwj"
      },
      "source": [
        "## Task 7. Utilize fallback responses\n",
        "\n",
        "When you are building a generative AI application, you may want to restrict the scope of what kinds of queries your application will respond to.\n",
        "\n",
        "A **fallback response** is a response the model should use when a user input would take the conversation out of your intended scope. It can provide the user a polite response that directs them back to the intended topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "GYBGc4xuY2JT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1050,
          "status": "ok",
          "timestamp": 1735206873699,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "GYBGc4xuY2JT",
        "outputId": "502fbde8-aa40-4ded-bb74-c0778e8f38f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sorry, I only talk about pottery!\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: How high can a horse jump?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1tUgAswpZGh6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2523,
          "status": "ok",
          "timestamp": 1735206963250,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "1tUgAswpZGh6",
        "outputId": "9b9e45fc-5fe8-44d8-819e-cc2c05e02735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The main difference between ceramic and porcelain is the type of clay used and the firing temperature. \n",
            "\n",
            "*Ceramic* is a general term for any object made from clay and hardened by heat. It can be made from a variety of clays, including earthenware, stoneware, and porcelain. \n",
            "\n",
            "*Porcelain*, on the other hand, is a specific type of ceramic made from kaolin clay, which is known for its white color and translucency. It is fired at a higher temperature than other ceramics, which gives it its characteristic strength and durability.\n"
          ]
        }
      ],
      "source": [
        "# You'll also want to test that the model does not reject on-topic questions, so make sure to try out\n",
        "# a variety of questions that the model should not decline to answer. Here is one:\n",
        "\n",
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Instructions: Answer questions about pottery.\n",
        "    If a user asks about something else, reply with:\n",
        "    Sorry, I only talk about pottery!\n",
        "\n",
        "    User Query: What is the difference between ceramic\n",
        "    and porcelain? Please keep your response brief.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g5wwXIX4ZUSk",
      "metadata": {
        "id": "g5wwXIX4ZUSk"
      },
      "source": [
        "## Task 8. Add contextual information\n",
        "\n",
        "Imagine you work for a grocery store chain and want to provide users a way of finding items in your store easily.\n",
        "\n",
        "Without contextual information, the model provides aisle numbers, but these are [hallucinations](https://cloud.google.com/gemini/docs/discover/responsible-ai#gemini-limitations) (or \"made up\" or \"imagined\" responses) because the model doesn't have the information it needs to provide an accurate response specific to your specific grocery store.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1ySNCgHoZbZ0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3252,
          "status": "ok",
          "timestamp": 1735207027926,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "1ySNCgHoZbZ0",
        "outputId": "8a671c51-3912-4b48-c1ce-bd1b1242993f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paper plates, mustard, and potatoes can typically be found in the following aisle numbers:\n",
            "\n",
            "**Paper plates:**\n",
            "\n",
            "* **Aisle 3 or 4:** Usually found in the paper goods aisle, alongside napkins, cups, and plastic utensils.\n",
            "\n",
            "**Mustard:**\n",
            "\n",
            "* **Aisle 6 or 7:** Typically located in the condiments aisle, near ketchup, mayonnaise, and other sauces.\n",
            "\n",
            "**Potatoes:**\n",
            "\n",
            "* **Aisle 8 or 9:** Generally found in the produce aisle, near other root vegetables like onions and carrots.\n",
            "\n",
            "However, aisle numbers can vary depending on the specific grocery store layout. It's always a good idea to check the store directory or ask an employee for assistance if you have trouble locating an item.\n",
            "\n",
            "Here are some helpful tips:\n",
            "\n",
            "* Look for overhead signs that indicate the aisle categories.\n",
            "* Use the store directory map, usually found near the entrance.\n",
            "* Ask a store employee for assistance.\n",
            "\n",
            "With these tips, finding paper plates, mustard, and potatoes at the grocery store should be a much easier task!\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "FC0TMj0aZ2hd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2481,
          "status": "ok",
          "timestamp": 1735207150553,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "FC0TMj0aZ2hd",
        "outputId": "da2d3a54-b7c5-4abc-8ead-e4c2a23c3128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Michael's Grocery Store Aisle Search:\n",
            "\n",
            "Based on your query, here's where you can find the items you listed at Michael's Grocery Store:\n",
            "\n",
            "* **Paper plates:** Aisle 17 (Household & Cleaning Supplies) is likely to have paper plates alongside other disposable tableware.\n",
            "* **Mustard:** Aisle 8 (Condiments & Spices) is where you'll find mustard with other condiments like ketchup, mayonnaise, and various sauces.\n",
            "* **Potatoes:** You have two options for potatoes:\n",
            "    * Aisle 2 (Vegetables): This is where you'll find fresh potatoes alongside other vegetables.\n",
            "    * Aisle 14 (Frozen Foods): If you're looking for frozen french fries or other frozen potato products, head to this aisle. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Now run a version of this prompt where you provide it that information in what is called the \"context\" part of the prompt:\n",
        "\n",
        "response = model.generate_content(\"\"\"\n",
        "    Context:\n",
        "    Michael's Grocery Store Aisle Layout:\n",
        "    Aisle 1: Fruits ‚Äî Apples, bananas,  grapes, oranges, strawberries, avocados, peaches, etc.\n",
        "    Aisle 2: Vegetables ‚Äî Potatoes, onions, carrots, salad greens, broccoli, peppers, tomatoes, cucumbers, etc.\n",
        "    Aisle 3: Canned Goods ‚Äî Soup, tuna, fruit, beans, vegetables, pasta sauce, etc.\n",
        "    Aisle 4: Dairy ‚Äî Butter, cheese, eggs, milk, yogurt, etc.\n",
        "    Aisle 5: Meat‚Äî Chicken, beef, pork, sausage, bacon etc.\n",
        "    Aisle 6: Fish & Seafood‚Äî Shrimp, crab, cod, tuna, salmon, etc.\n",
        "    Aisle 7: Deli‚Äî Cheese, salami, ham, turkey, etc.\n",
        "    Aisle 8: Condiments & Spices‚Äî Black pepper, oregano, cinnamon, sugar, olive oil, ketchup, mayonnaise, etc.\n",
        "    Aisle 9: Snacks‚Äî Chips, pretzels, popcorn, crackers, nuts, etc.\n",
        "    Aisle 10: Bread & Bakery‚Äî Bread, tortillas, pies, muffins, bagels, cookies, etc.\n",
        "    Aisle 11: Beverages‚Äî Coffee, teabags, milk, juice, soda, beer, wine, etc.\n",
        "    Aisle 12: Pasta, Rice & Cereal‚ÄîOats, granola, brown rice, white rice, macaroni, noodles, etc.\n",
        "    Aisle 13: Baking‚Äî Flour, powdered sugar, baking powder, cocoa etc.\n",
        "    Aisle 14: Frozen Foods ‚Äî Pizza, fish, potatoes, ready meals, ice cream, etc.\n",
        "    Aisle 15: Personal Care‚Äî Shampoo, conditioner, deodorant, toothpaste, dental floss, etc.\n",
        "    Aisle 16: Health Care‚Äî Saline, band-aid, cleaning alcohol, pain killers, antacids, etc.\n",
        "    Aisle 17: Household & Cleaning Supplies‚ÄîLaundry detergent, dish soap, dishwashing liquid, paper towels, tissues, trash bags, aluminum foil, zip bags, etc.\n",
        "    Aisle 18: Baby Items‚Äî Baby food, diapers, wet wipes, lotion, etc.\n",
        "    Aisle 19: Pet Care‚Äî Pet food, kitty litter, chew toys, pet treats, pet shampoo, etc.\n",
        "\n",
        "    Query:\n",
        "    On what aisle numbers can I find the following items?\n",
        "    - paper plates\n",
        "    - mustard\n",
        "    - potatoes\n",
        "    \"\"\")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aSOEf4zaJJN",
      "metadata": {
        "id": "7aSOEf4zaJJN"
      },
      "source": [
        "## Task 9. Structure prompts with prefixes or tags\n",
        "\n",
        "1. Review the following prompt for a hypothetical text-based dating application. It contains several prompt components, including:\n",
        "\n",
        "- defining a persona\n",
        "- specifying instructions\n",
        "- providing multiple pieces of contextual information for the main user and potential matches.\n",
        "\n",
        "2. Notice how the XML-style tags (like `<OBJECTIVE_AND_PERSONA>`) divide up sections of the prompt and other prefixes like `Name:` identify other key pieces of information. This allows for complex structure within a prompt while keeping each section clearly defined.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "Tr077M6maggM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3441,
          "status": "ok",
          "timestamp": 1735207447837,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "Tr077M6maggM",
        "outputId": "98645844-6f34-46f9-c68c-6530182777bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## Matchmaker Report for Allison:\n",
            "\n",
            "**Hey Allison!** I think I found a great match for you: **Felix**! \n",
            "\n",
            "Here's why I think you two would hit it off:\n",
            "\n",
            "* **Shared love for classical music:** You mentioned enjoying classical music concerts, and Felix is a huge Beethoven fan. Imagine attending concerts together and discussing your favorite pieces! \n",
            "* **Active lifestyles:** You enjoy swimming, and Felix used to play water polo and loves going to the beach. You both appreciate staying active and could explore new activities together, like kayaking or beach volleyball!\n",
            "* **Foodie connection:** While Felix loves German food, he mentioned making spaetzle, which are like German noodles. This aligns perfectly with your love for anything with noodles, especially Italian and ramen. You two could have fun trying different noodle dishes together!\n",
            "\n",
            "Overall, Felix seems like a perfect match for your interests and personality. He shares your passion for classical music, enjoys an active lifestyle, and loves exploring different cuisines. I highly recommend giving him a try! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "  <OBJECTIVE_AND_PERSONA>\n",
        "  You are a dating matchmaker.\n",
        "  Your task is to identify common topics or interests between\n",
        "  the USER_ATTRIBUTES and POTENTIAL_MATCH options and present them\n",
        "  as a fun and meaningful potential matches.\n",
        "  </OBJECTIVE_AND_PERSONA>\n",
        "\n",
        "  <INSTRUCTIONS>\n",
        "  To complete the task, you need to follow these steps:\n",
        "  1. Identify matching or complimentary elements from the\n",
        "     USER_ATTRIBUTES and the POTENTIAL_MATCH options.\n",
        "  2. Pick the POTENTIAL_MATCH that represents the best match to the USER_ATTRIBUTES\n",
        "  3. Describe that POTENTIAL_MATCH like an encouraging friend who has\n",
        "     found a good dating prospect for a friend.\n",
        "  4. Don't insult the user or potential matches.\n",
        "  5. Only mention the best match. Don't mention the other potential matches.\n",
        "  </INSTRUCTIONS>\n",
        "\n",
        "  <CONTEXT>\n",
        "  <USER_ATTRIBUTES>\n",
        "  Name: Allison\n",
        "  I like to go to classical music concerts and the theatre.\n",
        "  I like to swim.\n",
        "  I don't like sports.\n",
        "  My favorite cuisines are Italian and ramen. Anything with noodles!\n",
        "  </USER_ATTRIBUTES>\n",
        "\n",
        "  <POTENTIAL_MATCH 1>\n",
        "  Name: Jason\n",
        "  I'm very into sports.\n",
        "  My favorite team is the Detroit Lions.\n",
        "  I like baked potatoes.\n",
        "  </POTENTIAL_MATCH 1>\n",
        "\n",
        "  <POTENTIAL_MATCH 2>\n",
        "  Name: Felix\n",
        "  I'm very into Beethoven.\n",
        "  I like German food. I make a good spaetzle, which is like a German pasta.\n",
        "  I used to play water polo and still love going to the beach.\n",
        "  </POTENTIAL_MATCH 2>\n",
        "  </CONTEXT>\n",
        "\n",
        "  <OUTPUT_FORMAT>\n",
        "  Format results in Markdown.\n",
        "  </OUTPUT_FORMAT>\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MmbRYcnmbN9j",
      "metadata": {
        "id": "MmbRYcnmbN9j"
      },
      "source": [
        "## Task 10. Use system instructions\n",
        "\n",
        "You can include prompt components like those you've explored above in each call to a model, or you can pass them to the model upon instantiation as [system instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions#examples).\n",
        "\n",
        "_Note_: Content provided as a system instructions are billed as though they were passed in as part of each prompt at generation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "x8Tpho94b76s",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4735,
          "status": "ok",
          "timestamp": 1735207696209,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "x8Tpho94b76s",
        "outputId": "71c1dd0f-fd95-414a-d8bc-269d2cb90fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oh, my friend, what a delightful question for a music enthusiast like myself!  There's a whole universe of incredible musicians worth studying, each offering unique perspectives and innovations! \n",
            "\n",
            "Where should we even begin?  Do you have a particular instrument or genre in mind? For example, if you're interested in the piano, exploring the works of virtuosos like **Franz Liszt** for his dazzling showmanship or **Clara Schumann** for her poetic lyricism would be a fantastic starting point. \n",
            "\n",
            "Perhaps you're curious about the evolution of jazz?  Diving into the groundbreaking improvisations of **Louis Armstrong** or the complex harmonies of **Billie Holiday** could be absolutely captivating. \n",
            "\n",
            "Or maybe you're drawn to the storytelling prowess of folk music?  Discovering the poignant ballads of **Woody Guthrie** or the soulful melodies of **Odetta** might just sweep you off your feet! \n",
            "\n",
            "Tell me more about your musical inclinations, and I'll gladly guide you towards some truly remarkable artists!  üé∂ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Notice how the prompt passed to the generate_content() function doesn't mention music at all,\n",
        "# but the model still responds based on the persona & instructions passed to it as a system_instruction.\n",
        "\n",
        "system_instructions = \"\"\"\n",
        "    You will respond as a music historian,\n",
        "    demonstrating comprehensive knowledge\n",
        "    across diverse musical genres and providing\n",
        "    relevant examples. Your tone will be upbeat\n",
        "    and enthusiastic, spreading the joy of music.\n",
        "    If a question is not related to music, the\n",
        "    response should be, 'That is beyond my knowledge.'\n",
        "\"\"\"\n",
        "\n",
        "music_model = GenerativeModel(\"gemini-1.5-pro\", system_instruction=system_instructions)\n",
        "\n",
        "response = music_model.generate_content(\n",
        "    \"\"\"\n",
        "    Who is worth studying?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QiSz1y3KcHiz",
      "metadata": {
        "id": "QiSz1y3KcHiz"
      },
      "source": [
        "## Task 11. Demonstrate Chain-of-Thought\n",
        "\n",
        "Large language models predict what language should follow other language, but they cannot think through cause and effect in the world outside of language. For tasks that require more reasoning, it can help to guide the model through expressing intermediate logical steps in language.\n",
        "\n",
        "Large Language Models, especially Gemini, have gotten much better at reasoning on their own. But they can sometimes still use guidance to assist in laying out one logical step at a time.\n",
        "\n",
        "Gemini does a good job of demonstrating chain-of-thought by default, breaking a problem into multiple steps and then consolidating a final answer. But in a more complex case, providing an example with multiple intermediate steps written out clearly could make the difference between a correct or incorrect answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "wDyHXaXXdDnM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2614,
          "status": "ok",
          "timestamp": 1735207972600,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "wDyHXaXXdDnM",
        "outputId": "e24660e5-32bc-4990-e191-3458c3822be2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Today's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
            "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
            "\n",
            "Tomorrow's Production:\n",
            "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
            "* Medium efficiency factories: 1 factory * 60 units/day/factory = 60 units/day (Reconfigured factory)\n",
            "* Low efficiency factories: 1 factory * 30 units/day/factory * 0.5 = 15 units/day (Outaged factory)\n",
            "* **Total production today: 300 units/day + 60 units/day + 15 units/day = 375 units/day**\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = \"\"\"\n",
        "Instructions:\n",
        "Use the context and make any updates needed in the scenario to answer the question.\n",
        "\n",
        "Context:\n",
        "A high efficiency factory produces 100 units per day.\n",
        "A medium efficiency factory produces 60 units per day.\n",
        "A low efficiency factory produces 30 units per day.\n",
        "\n",
        "Megacorp owns 5 factories. 3 are high efficiency, 2 are low efficiency.\n",
        "\n",
        "<EXAMPLE SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will have to shut down one high efficiency factory.\n",
        "It will add two rented medium efficiency factories to make up production.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Today's Production:\n",
        "* High efficiency factories: 3 factories * 100 units/day/factory = 300 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 360 units/day**\n",
        "\n",
        "Tomorrow's Production:\n",
        "* High efficiency factories: 2 factories * 100 units/day/factory = 200 units/day\n",
        "* Medium efficiency factories: 2 factories * 60 units/day/factory = 120 units/day\n",
        "* Low efficiency factories: 2 factories * 30 units/day/factory = 60 units/day\n",
        "* **Total production today: 300 units/day + 60 units/day = 380 units/day**\n",
        "</EXAMPLE SCENARIO>\n",
        "\n",
        "<SCENARIO>\n",
        "Scenario:\n",
        "Tomorrow Megacorp will reconfigure a low efficiency factory up to medium efficiency.\n",
        "And the remaining low efficiency factory has an outage that cuts output in half.\n",
        "\n",
        "Question:\n",
        "How many units can they produce today? How many tomorrow?\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "response = model.generate_content(question, generation_config={\"temperature\": 0})\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fo4fyX_kdr4V",
      "metadata": {
        "id": "Fo4fyX_kdr4V"
      },
      "source": [
        "## Task 12. Break down complex tasks\n",
        "\n",
        "Often, complex tasks require multiple steps to work through them, even for us humans! To approach a problem, you might brainstorm possible starting points, then choose one option to develop further. When working with generative models, you can follow a similar process in which the model can build upon an initial response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "JHJPXV0BdyRD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 5985,
          "status": "ok",
          "timestamp": 1735208185038,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "JHJPXV0BdyRD",
        "outputId": "ce9746c2-f4ae-46a2-d255-5f90ab142d59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## 5 Metaphors to Explain the Difference Between TPUs and GPUs:\n",
            "\n",
            "1. **TPU as a Specialized Athlete vs. GPU as a Decathlete:**\n",
            "    * **TPU:** Imagine a marathoner, trained specifically for long-distance running. They excel at sustained workloads like machine learning training.\n",
            "    * **GPU:** Picture a decathlete, skilled in various athletic disciplines. They handle diverse tasks like gaming, video editing, and scientific simulations.\n",
            "\n",
            "2. **TPU as a Factory Assembly Line vs. GPU as a Multi-Tool:**\n",
            "    * **TPU:** A factory assembly line performs one specific task efficiently and at high volume, like training a single neural network model.\n",
            "    * **GPU:** A multi-tool has various functions but may not excel at any specific one. It handles diverse tasks, including gaming and video editing.\n",
            "\n",
            "3. **TPU as a Formula One Racecar vs. GPU as a Sports Car:**\n",
            "    * **TPU:** A Formula One car is built for speed and efficiency on a specific track, like training large language models.\n",
            "    * **GPU:** A sports car is versatile and performs well on various roads, like handling different gaming and design tasks.\n",
            "\n",
            "4. **TPU as a Deep Sea Diver vs. GPU as a Scuba Diver:**\n",
            "    * **TPU:** A deep-sea diver specializes in exploring the depths with specialized equipment, like training complex AI models.\n",
            "    * **GPU:** A scuba diver explores shallower waters with versatile gear, handling various tasks like gaming and video editing.\n",
            "\n",
            "5. **TPU as a Chess Supercomputer vs. GPU as a Home Computer:**\n",
            "    * **TPU:** A chess supercomputer is designed for one specific task - playing chess at the highest level. It excels at training complex AI models.\n",
            "    * **GPU:** A home computer performs various tasks, from word processing to web browsing. It handles diverse tasks but may not excel at any specific one.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    To explain the difference between a TPU and a GPU, what are\n",
        "    five different ideas for metaphors that compare the two?\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "brainstorm_response = response.text\n",
        "print(brainstorm_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "iePHkcl3eH0M",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3762,
          "status": "ok",
          "timestamp": 1735208259909,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "iePHkcl3eH0M",
        "outputId": "ac3fe441-1c4e-4983-f149-bf7a52c6e19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## TPU as a Formula One Racecar vs. GPU as a Sports Car\n",
            "\n",
            "This metaphor resonates with me because it effectively captures the contrasting strengths of TPUs and GPUs. Just like a Formula One car is built for speed and efficiency on a specific track, TPUs excel at training large language models and other AI applications with high computational demands. Their dedicated architecture and hardware optimizations enable them to perform specific tasks with unparalleled speed and efficiency.\n",
            "\n",
            "On the other hand, the GPU resembles a sports car, offering versatility and performance across a broad range of applications. Like a sports car navigating diverse terrains, GPUs handle various tasks ranging from gaming and video editing to scientific simulations. Their flexible architecture allows them to adapt to different workloads, making them a popular choice for diverse computing needs.\n",
            "\n",
            "This analogy helps me visualize the fundamental difference between TPUs and GPUs. While TPUs are like specialized athletes dominating a specific domain, GPUs are akin to multi-talented performers excelling in various fields. Understanding this distinction helps me choose the appropriate technology for my computing needs, depending on the task at hand.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    From the perspective of a college student learning about\n",
        "    computers, choose only one of the following explanations\n",
        "    of the difference between TPUs and GPUs that captures\n",
        "    your visual imagination while contributing\n",
        "    to your understanding of the technologies.\n",
        "\n",
        "    {brainstorm_response}\n",
        "    \"\"\".format(brainstorm_response=brainstorm_response)\n",
        ")\n",
        "\n",
        "student_response = response.text\n",
        "\n",
        "print(student_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "L4n5apoqeRhS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4465,
          "status": "ok",
          "timestamp": 1735208297161,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -660
        },
        "id": "L4n5apoqeRhS",
        "outputId": "0149367d-519b-47d9-a88e-97354eb2704b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## The Great Race: TPUs vs. GPUs in the AI Arena\n",
            "\n",
            "The world of AI is a high-octane arena, where training massive language models and complex algorithms resembles a thrilling Formula One race. In this competition for speed and efficiency, two titans clash: TPUs and GPUs, each with unique strengths and weaknesses, vying for the checkered flag.\n",
            "\n",
            "Imagine the TPU as a sleek, single-minded Formula One car, meticulously engineered for blistering speed on a specific track. Its dedicated architecture and hardware optimizations enable it to train large language models with unparalleled efficiency, crushing rivals on the straightaways of computational demands.\n",
            "\n",
            "On the other hand, the GPU resembles a versatile sports car, gracefully navigating diverse terrains. Its flexible architecture allows it to handle a wide range of tasks, from gaming and video editing to scientific simulations, excelling in a multitude of applications.\n",
            "\n",
            "This analogy illuminates the fundamental difference between these two AI powerhouses. While TPUs are specialized athletes dominating specific domains, GPUs are multi-talented performers, excelling in various fields. Choosing the right technology for your AI needs is like selecting the appropriate vehicle for the race you're competing in. To conquer the demanding track of large language model training, the TPU is your champion. If versatility and adaptability are your priorities, the GPU takes the lead.\n",
            "\n",
            "So, as the engines roar and the starting lights flash green, which AI powerhouse will emerge victorious in your specific race? Grasping the strengths and limitations of each, like a skilled race strategist, will help you cross the finish line with the checkered flag held high.\n"
          ]
        }
      ],
      "source": [
        "response = model.generate_content(\n",
        "    \"\"\"\n",
        "    Elaborate on the choice of metaphor below by turning\n",
        "    it into an introductory paragraph for a blog post.\n",
        "\n",
        "    {student_response}\n",
        "    \"\"\".format(student_response=student_response)\n",
        ")\n",
        "\n",
        "blog_post = response.text\n",
        "\n",
        "print(blog_post)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CY1nBzW0eaX6",
      "metadata": {
        "id": "CY1nBzW0eaX6"
      },
      "source": [
        "## Task 13. Implement prompt iteration strategies to improve your prompts version by version\n",
        "\n",
        "Your prompts may not always generate the results you have imagined on your first attempt.\n",
        "\n",
        "A few steps you can take to iterate on your prompts include:\n",
        "\n",
        "- Rephrasing the descriptions of your task, instructions, persona, or other prompt components.\n",
        "- Re-ordering the various components of the prompt to give the model a clue as early as possible as to what parts of the text you have provided are most relevant.\n",
        "- Breaking your task up into multiple, smaller tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "u4XtaZXBeqCO",
      "metadata": {
        "id": "u4XtaZXBeqCO"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "- Review the [overview of prompting strategies](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/prompt-design-strategies) from the Google Cloud documentation.\n",
        "- Review the guide to prompt design strategies from the Gemini API documentation.\n",
        "- Focus on the task-specific prompt guidance for prompts involving understanding images or video, code generation or interpretation prompts, image generation prompts, or other specific forms of prompts.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "student-02-930c17d67a23 (Dec 26, 2024, 8:32:54‚ÄØPM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

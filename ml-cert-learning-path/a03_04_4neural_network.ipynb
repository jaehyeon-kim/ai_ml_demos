{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a DNN using the Keras Functional API\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Review how to read in CSV file data using tf.data.\n",
    "2. Specify input, hidden, and output layers in the DNN architecture.\n",
    "3. Review and visualize the final DNN shape.\n",
    "4. Train the model locally and visualize the loss curves.\n",
    "5. Deploy and predict with the model using Cloud AI Platform. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will build a Keras DNN to predict the fare amount for NYC taxi cab rides.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/neural_network.ipynb) -- try to complete that notebook first before reviewing this solution notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use any Python source file as a module by executing an import statement in some other Python source file\n",
    "# The import statement combines two operations; it searches for the named module, then it binds the\n",
    "# results of that search to a name in the local scope.\n",
    "import os, json, math\n",
    "# Import data processing libraries like Numpy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Python shutil module enables us to operate with file objects easily and without diving into file objects a lot.\n",
    "import shutil\n",
    "# Show the currently installed version of TensorFlow\n",
    "print(\"TensorFlow version: \",tf.version.VERSION)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # SET TF ERROR LOG VERBOSITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locating the CSV files\n",
    "\n",
    "We will start with the CSV files that we wrote out in the other notebook. Just so you don't have to run the notebook, we saved a copy in ../data/toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ls` is a Linux shell command that lists directory contents\n",
    "# `l` flag list all the files with permissions and details\n",
    "!ls -l ../data/toy_data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 1: Use tf.data to read the CSV files\n",
    "\n",
    "\n",
    "First let's define our columns of data, which column we're predicting for, and the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of data\n",
    "CSV_COLUMNS  = ['fare_amount',  'pickup_datetime',\n",
    "                'pickup_longitude', 'pickup_latitude', \n",
    "                'dropoff_longitude', 'dropoff_latitude', \n",
    "                'passenger_count', 'key']\n",
    "LABEL_COLUMN = 'fare_amount'\n",
    "DEFAULTS     = [[0.0],['na'],[0.0],[0.0],[0.0],[0.0],[0.0],['na']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define our features we want to use and our label(s) and then load in the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features you want to use\n",
    "def features_and_labels(row_data):\n",
    "    for unwanted_col in ['pickup_datetime', 'key']:\n",
    "        row_data.pop(unwanted_col)\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "# load the training data\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "  dataset = (tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "             .map(features_and_labels) # features, label\n",
    "             )\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "  dataset = dataset.prefetch(1) # take advantage of multi-threading; 1=AUTOTUNE\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 2: Build a DNN with Keras\n",
    "\n",
    "Now let's build the Deep Neural Network (DNN) model in Keras and specify the input and hidden layers. We will print out the DNN architecture and then visualize it later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple Keras DNN using its Functional API\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true))) \n",
    "\n",
    "def build_dnn_model():\n",
    "    INPUT_COLS = ['pickup_longitude', 'pickup_latitude', \n",
    "                  'dropoff_longitude', 'dropoff_latitude', \n",
    "                  'passenger_count']\n",
    "\n",
    "    # TODO 2\n",
    "    # input layer\n",
    "    inputs = {\n",
    "        colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32')\n",
    "           for colname in INPUT_COLS\n",
    "    }\n",
    "    # tf.feature_column.numeric_column() represents real valued or numerical features.\n",
    "    feature_columns = {\n",
    "        colname : tf.feature_column.numeric_column(colname)\n",
    "           for colname in INPUT_COLS\n",
    "    }\n",
    "    \n",
    "    # the constructor for DenseFeatures takes a list of numeric columns\n",
    "    # The Functional API in Keras requires that you specify: LayerConstructor()(inputs)\n",
    "    dnn_inputs = tf.keras.layers.DenseFeatures(feature_columns.values())(inputs)\n",
    "\n",
    "    # two hidden layers of [32, 8] just in like the BQML DNN\n",
    "    h1 = tf.keras.layers.Dense(32, activation='relu', name='h1')(dnn_inputs)\n",
    "    h2 = tf.keras.layers.Dense(8, activation='relu', name='h2')(h1)\n",
    "\n",
    "    # final output is a linear activation because this is regression\n",
    "    output = tf.keras.layers.Dense(1, activation='linear', name='fare')(h2)\n",
    "    model = tf.keras.models.Model(inputs, output)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[rmse, 'mse'])\n",
    "    return model\n",
    "\n",
    "print(\"Here is our DNN architecture so far:\\n\")\n",
    "model = build_dnn_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 3: Visualize the DNN\n",
    "\n",
    "We can visualize the DNN using the Keras [plot_model](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/utils/plot_model) utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model() Converts a Keras model to dot format and save to a file.\n",
    "tf.keras.utils.plot_model(model, 'dnn_model.png', show_shapes=False, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 4: Train the model\n",
    "\n",
    "To train the model, simply call [model.fit()](https://keras.io/models/model/#fit).\n",
    "\n",
    "Note that we should really use many more NUM_TRAIN_EXAMPLES (i.e. a larger dataset). We shouldn't make assumptions about the quality of the model based on training/evaluating it on a small sample of the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "NUM_TRAIN_EXAMPLES = 10000 * 5 # training dataset repeats, so it will wrap around\n",
    "NUM_EVALS = 32  # how many times to evaluate\n",
    "NUM_EVAL_EXAMPLES = 10000 # enough to get a reasonable sample, but not so much that it slows down\n",
    "\n",
    "trainds = load_dataset('../data/toy_data/taxi-traffic-train*', TRAIN_BATCH_SIZE, tf.estimator.ModeKeys.TRAIN)\n",
    "evalds = load_dataset('../data/toy_data/taxi-traffic-valid*', 1000, tf.estimator.ModeKeys.EVAL).take(NUM_EVAL_EXAMPLES//1000)\n",
    "\n",
    "steps_per_epoch = NUM_TRAIN_EXAMPLES // (TRAIN_BATCH_SIZE * NUM_EVALS)\n",
    "\n",
    "# Model Fit\n",
    "history = model.fit(trainds, \n",
    "                    validation_data=evalds,\n",
    "                    epochs=NUM_EVALS, \n",
    "                    steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model loss curve\n",
    "\n",
    "Next, we will use matplotlib to draw the model's loss curves for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "# Use matplotlib for visualizing the model\n",
    "import matplotlib.pyplot as plt\n",
    "nrows = 1\n",
    "ncols = 2\n",
    "# The .figure() method will create a new figure, or activate an existing figure.\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for idx, key in enumerate(['loss', 'rmse']):\n",
    "    ax = fig.add_subplot(nrows, ncols, idx+1)\n",
    "# The .plot() is a versatile function, and will take an arbitrary number of arguments. For example, to plot x versus y.\n",
    "    plt.plot(history.history[key])\n",
    "    plt.plot(history.history['val_{}'.format(key)])\n",
    "# The .title() method sets a title for the axes.\n",
    "    plt.title('model {}'.format(key))\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel('epoch')\n",
    "# The .legend() method will place a legend on the axes.\n",
    "    plt.legend(['train', 'validation'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task 5: Predict with the model locally\n",
    "\n",
    "To predict with Keras, you simply call [model.predict()](https://keras.io/models/model/#predict) and pass in the cab ride you want to predict the fare amount for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5\n",
    "# Use the model to do prediction with `model.predict()`\n",
    "model.predict({\n",
    "    'pickup_longitude': tf.convert_to_tensor([-73.982683]),\n",
    "    'pickup_latitude': tf.convert_to_tensor([40.742104]),\n",
    "    'dropoff_longitude': tf.convert_to_tensor([-73.983766]),\n",
    "    'dropoff_latitude': tf.convert_to_tensor([40.755174]),\n",
    "    'passenger_count': tf.convert_to_tensor([3.0]),    \n",
    "}, steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this is not realistic, because we can't expect client code to have a model object in memory. We'll have to export our model to a file, and expect client code to instantiate the model from that exported file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
